{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Validator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the implementation of custom validators easier, we have provided [an interface in the OSS](https://github.com/guardrails-ai/guardrails/blob/main/guardrails/validator_base.py).\n",
    "\n",
    "There are a few key steps to get up and running with a custom validator:\n",
    "\n",
    "1. Implementing the validator\n",
    "2. Conforming to the required interface\n",
    "3. Running Locally/Submitting to the Validator Hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's build out a simple custom validator. This validator will check if all text in the input is lowercase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "import requests\n",
    "from guardrails.validator_base import (\n",
    "    FailResult,\n",
    "    PassResult,\n",
    "    ValidationResult,\n",
    "    Validator,\n",
    "    register_validator,\n",
    "    ErrorSpan,\n",
    ")\n",
    "from typing import Optional, Callable\n",
    "\n",
    "\n",
    "@register_validator(name=\"guardrails/lower_case\", data_type=\"string\")\n",
    "class LowercaseValidator(Validator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        on_fail: Optional[Callable] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(on_fail=on_fail, **kwargs)\n",
    "        self.rail_alias = \"lowercase\"\n",
    "\n",
    "    def _validate(self, value: Any, metadata: Dict[str, Any]) -> ValidationResult:\n",
    "        if not isinstance(value, str):\n",
    "            return FailResult(\n",
    "                metadata=metadata,\n",
    "                error_message=\"Input must be a string.\",\n",
    "                fix_value=None,\n",
    "            )\n",
    "\n",
    "        inference_result = self._inference(value)\n",
    "\n",
    "        if inference_result:\n",
    "            return PassResult()\n",
    "        else:\n",
    "            return FailResult(\n",
    "                metadata=metadata,\n",
    "                error_message=\"Input must be lowercase.\",\n",
    "                fix_value=value.lower(),\n",
    "            )\n",
    "\n",
    "    def _inference_local(self, model_input: str) -> bool:\n",
    "        \"\"\"Implement a function to perform inference on a local machine.\"\"\"\n",
    "        return model_input.islower()\n",
    "\n",
    "    def _inference_remote(self, model_input: str) -> bool:\n",
    "        \"\"\"Implement a function that will build a request and perform inference on a\n",
    "        remote machine. This is not required if you will always use local mode.\n",
    "        \"\"\"\n",
    "        response = requests.post(self.validation_endpoint, json={\"inputs\": model_input})\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\n",
    "                f\"Remote inference failed with status code {response.status_code}\"\n",
    "            )\n",
    "\n",
    "        return response.json().get(\"is_lowercase\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple usage running locally\n",
    "\n",
    "If this validator was stored locally in your codebase, you would want to import it\n",
    "\n",
    "```bash\n",
    "from lowercase_validator import LowercaseValidator\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "lowercase_validator = LowercaseValidator(use_local=True)\n",
    "\n",
    "\n",
    "# Test cases for local validator\n",
    "print(lowercase_validator.validate(value=\"hello world\", metadata={}))  # PassResult\n",
    "print(lowercase_validator.validate(\"Hello World\", {}))  # FailResult\n",
    "print(\n",
    "    lowercase_validator.validate(\"123\", {})\n",
    ")  # PassResult (numbers are considered lowercase)\n",
    "print(lowercase_validator.validate(123, {}))  # FailResult (not a string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best practice would be to use the validator in a Guardrails.Guard()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some imports\n",
    "from rich import print\n",
    "import guardrails as gd\n",
    "import litellm\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Tell me a short snippet about the company Apple.\n",
    "Make sure the output is all in lowercase. Don't use any capital letters.\"\"\"\n",
    "\n",
    "guard = gd.Guard().use(LowercaseValidator, use_local=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a guard object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = fragment_generator = guard(\n",
    "    litellm.completion,\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    max_tokens=1024,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like an improved implementation, you can implement the new ErrorSpan feature. This class provides a way to define the span of the error in the input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_validator(name=\"guardrails/lower_case\", data_type=\"string\")\n",
    "class LowercaseValidator(Validator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        on_fail: Optional[Callable] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(on_fail=on_fail, **kwargs)\n",
    "        self.rail_alias = \"lowercase\"\n",
    "\n",
    "    def _validate(self, value: Any, metadata: Dict[str, Any]) -> ValidationResult:\n",
    "        if not isinstance(value, str):\n",
    "            return FailResult(error_message=\"Input must be a string.\", fix_value=None)\n",
    "\n",
    "        inference_result = self._inference(value)\n",
    "        error_spans = []\n",
    "        for result in inference_result:\n",
    "            error_spans.append(\n",
    "                ErrorSpan(\n",
    "                    start=result[0], end=result[1], reason=\"Input must be lowercase.\"\n",
    "                )\n",
    "            )\n",
    "        return FailResult(\n",
    "            error_message=\"Input must be lowercase.\",\n",
    "            fix_value=value.lower(),\n",
    "            error_spans=error_spans,\n",
    "        )\n",
    "\n",
    "    def _inference_local(self, model_input: str) -> bool:\n",
    "        \"\"\"Implement a function to perform inference on a local machine.\"\"\"\n",
    "        error_spans = []\n",
    "        start = None\n",
    "\n",
    "        for i, char in enumerate(model_input):\n",
    "            if char.isupper():\n",
    "                if start is None:\n",
    "                    start = i\n",
    "            elif start is not None:\n",
    "                error_spans.append((start, i - 1))\n",
    "                start = None\n",
    "\n",
    "        if start is not None:\n",
    "            error_spans.append((start, len(model_input) - 1))\n",
    "\n",
    "        return error_spans\n",
    "\n",
    "    def _inference_remote(self, model_input: str) -> bool:\n",
    "        \"\"\"Implement a function that will build a request and perform inference on a\n",
    "        remote machine. This is not required if you will always use local mode.\n",
    "        \"\"\"\n",
    "        response = requests.post(self.validation_endpoint, json={\"inputs\": model_input})\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\n",
    "                f\"Remote inference failed with status code {response.status_code}\"\n",
    "            )\n",
    "\n",
    "        return response.json().get(\"is_lowercase\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guard = gd.Guard().use(LowercaseValidator, use_local=True)\n",
    "\n",
    "result = fragment_generator = guard(\n",
    "    litellm.completion,\n",
    "    prompt=prompt,\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    max_tokens=1024,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
