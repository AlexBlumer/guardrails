{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Async Stream-validate LLM responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few imports and global variables\n",
    "from rich import print\n",
    "import guardrails as gd\n",
    "import litellm\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Install the necessary validators from Guardrails hub in your CLI.\n",
    "\n",
    "```bash\n",
    "!guardrails hub install hub://guardrails/competitor_check\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Guard object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CompetitorCheck' from 'guardrails.hub' (/Users/wyatt/Projects/guardrails/guardrails/hub/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mguardrails\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompetitorCheck\n\u001b[1;32m      2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me about the Apple Iphone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m guard \u001b[38;5;241m=\u001b[39m gd\u001b[38;5;241m.\u001b[39mAsyncGuard\u001b[38;5;241m.\u001b[39muse(CompetitorCheck, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApple\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CompetitorCheck' from 'guardrails.hub' (/Users/wyatt/Projects/guardrails/guardrails/hub/__init__.py)"
     ]
    }
   ],
   "source": [
    "from guardrails.hub import CompetitorCheck\n",
    "prompt = \"Tell me about the Apple Iphone\"\n",
    "\n",
    "guard = gd.AsyncGuard.use(CompetitorCheck, [\"Apple\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1: No async streaming\n",
    "\n",
    "By default, the `stream` parameter is set to `False`. \n",
    "We will use LiteLLM to make our LLM calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the litellm OpenAI API call with the `guard` object\n",
    "raw_llm_output, validated_output, *rest = await guard(\n",
    "    litellm.acompletion,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    prompt=prompt,\n",
    "    max_tokens=1024,\n",
    "    temperature=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the logs\n",
    "print(guard.history.last.tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2: Async Streaming\n",
    "\n",
    "Set the `stream` parameter to `True`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the litellm OpenAI API call with the `guard` object\n",
    "fragment_generator = await guard(\n",
    "    litellm.acompletion,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    prompt=prompt,\n",
    "    max_tokens=1024,\n",
    "    temperature=0,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "\n",
    "async for op in fragment_generator:\n",
    "    clear_output(wait=True)\n",
    "    print(op)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the logs\n",
    "print(guard.history.last.tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here, the outputs in both examples match. The only difference is that, in the async streaming example, the outputs are returned as soon as they are received and validated by Guardrails.\n",
    "\n",
    " In the non-streaming example, the outputs are returned only after the entire request has been processed by the API. \n",
    " \n",
    " In other words, when async streaming is enabled, the API returns the outputs as soon as they are ready, rather than waiting for the entire request to be processed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guard-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
