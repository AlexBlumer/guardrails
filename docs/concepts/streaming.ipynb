{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "In production systems with user interaction, streaming output from LLMs greatly improves the user experience. Streaming allows you to build real-time systems that minimize the time to first token (TTFT) rather than waiting for the entire document to be completed before progessing.\n",
       "\n",
       "\n",
       "Guardrails natively supports validation for streaming output, supporting both synchronous and asynchronous approaches."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "from rich import print\n",
       "import guardrails as gd\n",
       "import litellm\n",
       "from IPython.display import clear_output\n",
       "import time"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "Streaming with a guard class can be done by setting the 'stream' parameter to 'True'"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "guard = gd.Guard()\n",
       "\n",
       "fragment_generator = guard(\n",
       "    model=\"gpt-4o\",\n",
       "    messages=[\n",
       "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
       "        {\"role\": \"user\", \"content\": \"Tell me about LLM streaming APIs.\"},\n",
       "    ],\n",
       "    max_tokens=1024,\n",
       "    temperature=0,\n",
       "    stream=True,\n",
       ")\n",
       "\n",
       "\n",
       "for op in fragment_generator:\n",
       "    clear_output(wait=True)\n",
       "    print(op)\n",
       "    time.sleep(0.5)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "In cases where concurrent network calls are happening (many LLM calls!) it may be beneficial to use an asynchronous LLM client. Guardrails also natively supports asynchronous streaming calls."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "guard = gd.AsyncGuard()\n",
       "\n",
       "fragment_generator = await guard(\n",
       "    model=\"gpt-3.5-turbo\",\n",
       "    messages=[\n",
       "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
       "        {\"role\": \"user\", \"content\": \"Tell me about the streaming API of guardrails.\"},\n",
       "    ],\n",
       "    max_tokens=1024,\n",
       "    temperature=0,\n",
       "    stream=True,\n",
       ")\n",
       "\n",
       "\n",
       "async for op in fragment_generator:\n",
       "    clear_output(wait=True)\n",
       "    print(op)\n",
       "    time.sleep(0.5)"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 2
   }
   