{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Async Stream-validate LLM responses\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Few imports and global variables\n",
       "from rich import print\n",
       "import guardrails as gd\n",
       "import litellm\n",
       "from IPython.display import clear_output\n",
       "import time"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Setup\n",
       "\n",
       "Install the necessary validators from Guardrails hub in your CLI.\n",
       "\n",
       "```bash\n",
       "!guardrails hub install hub://guardrails/valid_range\n",
       "!guardrails hub install hub://guardrails/uppercase\n",
       "!guardrails hub install hub://guardrails/lowercase\n",
       "!guardrails hub install hub://guardrails/one_line\n",
       "```"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### 1. For structured JSON output\n"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "#### Define the prompt and output schema\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "from pydantic import BaseModel, Field\n",
       "from guardrails.hub import LowerCase, UpperCase, ValidRange, OneLine\n",
       "from typing import List\n",
       "\n",
       "prompt = \"\"\"\n",
       "Given the following doctor's notes about a patient, please extract a dictionary that contains the patient's information.\n",
       "\n",
       "${doctors_notes}\n",
       "\n",
       "${gr.complete_json_suffix_v2}\n",
       "\"\"\"\n",
       "\n",
       "doctors_notes = \"\"\"152 y/o female with chronic macular rash to face and hair, worse in beard, eyebrows and nares.\n",
       "The rash is itchy, flaky and slightly scaly. Moderate response to OTC steroid cream. Patient has been using cream for 2 weeks and also suffers from diabetes.\"\"\"\n",
       "\n",
       "\n",
       "class Symptom(BaseModel):\n",
       "    symptom: str = Field(description=\"Symptom that a patient is experiencing\")\n",
       "    affected_area: str = Field(\n",
       "        description=\"What part of the body the symptom is affecting\",\n",
       "        validators=[\n",
       "            LowerCase(on_fail=\"fix\"),\n",
       "        ],\n",
       "    )\n",
       "\n",
       "\n",
       "class Medication(BaseModel):\n",
       "    medication: str = Field(\n",
       "        description=\"Name of the medication the patient is taking\",\n",
       "        validators=[UpperCase(on_fail=\"fix\")],\n",
       "    )\n",
       "    response: str = Field(description=\"How the patient is responding to the medication\")\n",
       "\n",
       "\n",
       "class PatientInfo(BaseModel):\n",
       "    gender: str = Field(description=\"Patient's gender\")\n",
       "    age: int = Field(\n",
       "        description=\"Patient's age\",\n",
       "        validators=[ValidRange(min=0, max=100, on_fail=\"fix\")],\n",
       "    )\n",
       "    symptoms: List[Symptom] = Field(\n",
       "        description=\"Symptoms that the patient is currently experiencing. Each symptom should be classified into  separate item in the list.\"\n",
       "    )\n",
       "    current_meds: List[Medication] = Field(\n",
       "        description=\"Medications the patient is currently taking and their response\"\n",
       "    )\n",
       "    miscellaneous: str = Field(\n",
       "        description=\"Any other information that is relevant to the patient's health; something that doesn't fit into the other categories.\",\n",
       "        validators=[LowerCase(on_fail=\"fix\"), OneLine(on_fail=\"fix\")],\n",
       "    )"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "#### Create the Guard object\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "guard = gd.AsyncGuard.from_pydantic(output_class=PatientInfo, prompt=prompt)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "##### Example 1: No async streaming\n",
       "\n",
       "By default, the `stream` parameter is set to `False`. \n",
       "We will use LiteLLM to make our LLM calls.\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Wrap the litellm OpenAI API call with the `guard` object\n",
       "raw_llm_output, validated_output, *rest = await guard(\n",
       "    litellm.acompletion,\n",
       "    model=\"gpt-3.5-turbo\",\n",
       "    prompt_params={\"doctors_notes\": doctors_notes},\n",
       "    max_tokens=1024,\n",
       "    temperature=0.3,\n",
       ")\n",
       "\n",
       "# Print the validated output from the LLM\n",
       "print(validated_output)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Let's see the logs\n",
       "print(guard.history.last.tree)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "##### Example 2: Async Streaming\n",
       "\n",
       "Set the `stream` parameter to `True`\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Wrap the litellm OpenAI API call with the `guard` object\n",
       "fragment_generator = await guard(\n",
       "    litellm.acompletion,\n",
       "    model=\"gpt-3.5-turbo\",\n",
       "    prompt_params={\"doctors_notes\": doctors_notes},\n",
       "    max_tokens=1024,\n",
       "    temperature=0,\n",
       "    stream=True,\n",
       ")\n",
       "\n",
       "\n",
       "async for op in fragment_generator:\n",
       "    clear_output(wait=True)\n",
       "    print(op)\n",
       "    time.sleep(0.5)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Let's see the logs\n",
       "print(guard.history.last.tree)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "As you can see here, the outputs in both examples match. The only difference is that, in the async streaming example, the outputs are returned as soon as they are received and validated by Guardrails. In the non-streaming example, the outputs are returned only after the entire request has been processed by the API. In other words, when async streaming is enabled, the API returns the outputs as soon as they are ready, rather than waiting for the entire request to be processed.\n"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### 2. For unstructured text output\n"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "#### Define the prompt and Guard object with validators\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "from guardrails.hub import UpperCase, OneLine\n",
       "\n",
       "prompt = \"\"\"\n",
       "Generate a short description of large language models. Each new sentence should be on another line.\n",
       "\"\"\"\n",
       "\n",
       "guard = gd.AsyncGuard.from_string(\n",
       "    validators=[\n",
       "        UpperCase(on_fail=\"fix\"),\n",
       "        OneLine(on_fail=\"fix\"),\n",
       "    ],\n",
       "    description=\"testmeout\",\n",
       "    prompt=prompt,\n",
       ")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "#### Example 1: No async streaming\n",
       "\n",
       "By default, the `stream` parameter is set to `False`\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Wrap the litellm OpenAI API call with the `guard` object\n",
       "raw, validated, *rest = await guard(\n",
       "    litellm.acompletion,\n",
       "    model=\"gpt-3.5-turbo\",\n",
       "    max_tokens=50,\n",
       "    temperature=0.1,\n",
       ")\n",
       "\n",
       "# Print the raw and validated outputs\n",
       "print(f\"Raw output:\\n{raw}\")\n",
       "print(f\"Validated output:\\n{validated}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "#### Example 2: With async streaming\n",
       "\n",
       "Set the `stream` parameter to `True`\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Wrap the litellm OpenAI API call with the `guard` object\n",
       "fragment_generator = await guard(\n",
       "    litellm.acompletion,\n",
       "    model=\"gpt-3.5-turbo\",\n",
       "    max_tokens=50,\n",
       "    temperature=0.1,\n",
       "    stream=True,\n",
       ")\n",
       "\n",
       "\n",
       "async for op in fragment_generator:\n",
       "    clear_output(wait=True)\n",
       "    print(op)\n",
       "    time.sleep(0.1)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# See guard history\n",
       "print(guard.history.last.tree)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "As you can see, the outputs in both examples match. The only difference is that, in the streaming example, the outputs are returned as soon as they are received and validated by Guardrails. In the non-streaming example, the outputs are returned only after the entire request has been processed by the API. In other words, when streaming is enabled, the API returns the outputs as soon as they are ready, rather than waiting for the entire request to be processed."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "guard-venv",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 2
   }
   